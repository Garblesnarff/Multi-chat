

reasoning-wizard


GitHub

Publish
I want to build a chatbot, with a clean dashboard. The dashboard should be modern, clean, and have a 'cool' color gradient. 
<models>
This chatbot will use multiple LLM providers, and api's.

From Groq.com we will use:
llama-3.1-8b-instant (8k token context window)
llama-3.1-70b-versatile (8k token context window)
mixtral-8x7b-32768 (32k token context window)

We will also use googles gemini model:

gemini-1.5-flash-8b-exp-0827: (1 million token context window)
</models>
<info>
<gemini_info>
Gemini API quickstart:

Prerequisites
This quickstart assumes that you're familiar with building applications with Python.

To complete this quickstart, ensure that your development environment meets the following requirements:

Python 3.9+
Install the Gemini API SDK
The Python SDK for the Gemini API is contained in the google-generativeai package. Install the dependency using pip:


pip install -q -U google-generativeai
Set up your API key
To use the Gemini API, you'll need an API key. If you don't already have one, create a key in Google AI Studio.

Get an API key from Google AI Studio

Then, configure your key.

It is strongly recommended that you do not check an API key into your version control system but assign it as an environment variable instead:


export API_KEY=<YOUR_API_KEY>
Import the library
Import and configure the Google Generative AI library.


import google.generativeai as genai
import os

genai.configure(api_key=os.environ["API_KEY"])
Make your first request
Use the generateContent method to generate text.


model = genai.GenerativeModel("gemini-1.5-flash")
response = model.generate_content("Write a story about a magic backpack.")
print(response.text)

</gemini_info>
<groq_info>
Chat
Create chat completion

POSThttps://api.groq.com/openai/v1/chat/completions

Creates a model response for the given chat conversation.
Request Body

    frequency_penaltynumber or nullOptionalDefaults to 0

    Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    function_callDeprecatedstring / object or nullOptional

    Deprecated in favor of tool_choice.

    Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {"name": "my_function"} forces the model to call that function.

    none is the default when no functions are present. auto is the default if functions are present.

functionsDeprecatedarray or nullOptional

Deprecated in favor of tools.

A list of functions the model may generate JSON inputs for.
logit_biasobject or nullOptionalDefaults to null

This is not yet supported by any of our models. Modify the likelihood of specified tokens appearing in the completion.
logprobsboolean or nullOptionalDefaults to false

This is not yet supported by any of our models. Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
max_tokensinteger or nullOptional

The maximum number of tokens that can be generated in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.
messagesarrayRequired

A list of messages comprising the conversation so far.
modelstringRequired

ID of the model to use. For details on which models are compatible with the Chat API, see available models
ninteger or nullOptionalDefaults to 1

How many chat completion choices to generate for each input message. Note that the current moment, only n=1 is supported. Other values will result in a 400 response.
parallel_tool_callsboolean or nullOptionalDefaults to true

Whether to enable parallel function calling during tool use.
presence_penaltynumber or nullOptionalDefaults to 0

Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
response_formatobject or nullOptional

An object specifying the format that the model must output.

Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message.
seedinteger or nullOptional

If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.
stopstring / array or nullOptionalDefaults to null

Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
streamboolean or nullOptionalDefaults to false

If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Example code.
stream_optionsobject or nullOptionalDefaults to null

Options for streaming response. Only set this when you set stream: true.
temperaturenumber or nullOptionalDefaults to 1

What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both
tool_choicestring / object or nullOptional

Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool.

none is the default when no tools are present. auto is the default if tools are present.
toolsarray or nullOptional

A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.

    top_logprobsinteger or nullOptional

    This is not yet supported by any of our models. An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
    top_pnumber or nullOptionalDefaults to 1

    An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.
    userstring or nullOptional

    A unique identifier representing your end-user, which can help us monitor and detect abuse.

Returns

Returns a chat completion object, or a streamed sequence of chat completion chunk objects if the request is streamed.
</groq_info>
<info>

<code_to_adapt>
<app/api_handlers.py>
import json
import requests
import groq
import time
from abc import ABC, abstractmethod

class BaseHandler(ABC):
    def __init__(self):
        self.max_attempts = 3
        self.retry_delay = 1

    @abstractmethod
    def _make_request(self, messages, max_tokens):
        pass

    def make_api_call(self, messages, max_tokens, is_final_answer=False):
        for attempt in range(self.max_attempts):
            try:
                response = self._make_request(messages, max_tokens)
                return self._process_response(response, is_final_answer)
            except Exception as e:
                if attempt == self.max_attempts - 1:
                    return self._error_response(str(e), is_final_answer)
                time.sleep(self.retry_delay)

    def _process_response(self, response, is_final_answer):
        return json.loads(response)

    def _error_response(self, error_msg, is_final_answer):
        return {
            "title": "Error",
            "content": f"Failed to generate {'final answer' if is_final_answer else 'step'} after {self.max_attempts} attempts. Error: {error_msg}",
            "next_action": "final_answer" if is_final_answer else "continue"
        }

class OllamaHandler(BaseHandler):
    def __init__(self, url, model):
        super().__init__()
        self.url = url
        self.model = model

    def _make_request(self, messages, max_tokens):
        response = requests.post(
            f"{self.url}/api/chat",
            json={
                "model": self.model,
                "messages": messages,
                "stream": False,
                "format": "json",
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.2
                }
            }
        )
        response.raise_for_status()
        return response.json()["message"]["content"]

class PerplexityHandler(BaseHandler):
    def __init__(self, api_key, model):
        super().__init__()
        self.api_key = api_key
        self.model = model

    def _clean_messages(self, messages):
        cleaned_messages = []
        last_role = None
        for message in messages:
            if message["role"] == "system":
                cleaned_messages.append(message)
            elif message["role"] != last_role:
                cleaned_messages.append(message)
                last_role = message["role"]
            elif message["role"] == "user":
                cleaned_messages[-1]["content"] += "\n" + message["content"]
        # If the last message is an assistant message, delete it
        if cleaned_messages and cleaned_messages[-1]["role"] == "assistant":
            cleaned_messages.pop()  
        return cleaned_messages

    def _make_request(self, messages, max_tokens):
        cleaned_messages = self._clean_messages(messages)

        url = "https://api.perplexity.ai/chat/completions"
        payload = {"model": self.model, "messages": cleaned_messages}
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except requests.exceptions.HTTPError as http_err:
            if response.status_code == 400:
                error_message = response.json().get("error", {}).get("message", "Unknown error")
                raise ValueError(f"Bad request (400): {error_message}")
            raise  # Re-raise the exception if it's not a 400 error

    def _process_response(self, response, is_final_answer):
        try:
            return super()._process_response(response, is_final_answer)
        except json.JSONDecodeError:
            print("Warning: content is not a valid JSON, returning raw response")
            forced_final_answer = '"next_action": "final_answer"' in response.lower().strip()
            return {
                "title": "Raw Response",
                "content": response,
                "next_action": "final_answer" if (is_final_answer or forced_final_answer) else "continue"
            }

class GroqHandler(BaseHandler):
    def __init__(self):
        super().__init__()
        self.client = groq.Groq()

    def _make_request(self, messages, max_tokens):
        response = self.client.chat.completions.create(
            model="llama-3.1-70b-versatile",
            messages=messages,
            max_tokens=max_tokens,
            temperature=0.2,
            response_format={"type": "json_object"}
        )
        return response.choices[0].message.content
</app/api_handlers.py>

<app/config_menu>
import streamlit as st
import os
from dotenv import load_dotenv, set_key

def load_env_vars():
    load_dotenv(os.path.join(os.path.dirname(__file__), "..", ".env"))
    return {
        'OLLAMA_URL': os.getenv('OLLAMA_URL', 'http://localhost:11434'),
        'OLLAMA_MODEL': os.getenv('OLLAMA_MODEL', 'mistral'),
        'PERPLEXITY_API_KEY': os.getenv('PERPLEXITY_API_KEY', ''),
        'PERPLEXITY_MODEL': os.getenv('PERPLEXITY_MODEL', 'mistral-7b-instruct'),
        'GROQ_API_KEY': os.getenv('GROQ_API_KEY', ''),
        'GROQ_MODEL': os.getenv('GROQ_MODEL', 'mixtral-8x7b-32768')
    }

def save_env_vars(config):
    env_path = os.path.join(os.path.dirname(__file__), "..", ".env")
    for key, value in config.items():
        set_key(env_path, key, value)

def config_menu():
    st.sidebar.markdown("## 🛠️ Configuration")
    
    config = load_env_vars()
    
    with st.sidebar.expander("Edit Configuration"):
        new_config = {}
        new_config['OLLAMA_URL'] = st.text_input("Ollama URL", value=config['OLLAMA_URL'])
        new_config['OLLAMA_MODEL'] = st.text_input("Ollama Model", value=config['OLLAMA_MODEL'])
        new_config['PERPLEXITY_API_KEY'] = st.text_input("Perplexity API Key", value=config['PERPLEXITY_API_KEY'], type="password")
        new_config['PERPLEXITY_MODEL'] = st.text_input("Perplexity Model", value=config['PERPLEXITY_MODEL'])
        new_config['GROQ_API_KEY'] = st.text_input("Groq API Key", value=config['GROQ_API_KEY'], type="password")
        new_config['GROQ_MODEL'] = st.text_input("Groq Model", value=config['GROQ_MODEL'])
        
        if st.button("Save Configuration"):
            save_env_vars(new_config)
            st.success("Configuration saved successfully!")
    
    return config

def display_config(backend, config):
    st.sidebar.markdown("## 🛠️ Current Configuration")
    if backend == "Ollama":
        st.sidebar.markdown(f"- 🖥️ Ollama URL: `{config['OLLAMA_URL']}`")
        st.sidebar.markdown(f"- 🤖 Ollama Model: `{config['OLLAMA_MODEL']}`")
    elif backend == "Perplexity AI":
        st.sidebar.markdown(f"- 🧠 Perplexity AI Model: `{config['PERPLEXITY_MODEL']}`")
    else:  # Groq
        st.sidebar.markdown(f"- ⚡ Groq Model: `{config['GROQ_MODEL']}`")
</app/config_menu.py>

<app/logger.py>
import logging
import os
from datetime import datetime

def setup_logger():
    # Create a logs directory if it doesn't exist
    log_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'logs')
    os.makedirs(log_dir, exist_ok=True)

    # Create a unique log file name based on the current timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"multi1_{timestamp}.log")

    # Configure the logger
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()  # This will also print logs to console
        ]
    )

    return logging.getLogger('multi1')

# Create a global logger instance
logger = setup_logger()
</app/logger.py>

<app/main.py>
import streamlit as st
from dotenv import load_dotenv
from api_handlers import OllamaHandler, PerplexityHandler, GroqHandler
from utils import generate_response
from config_menu import config_menu, display_config
from logger import logger
import os

# Load environment variables
load_dotenv()

def load_css():
    with open(os.path.join(os.path.dirname(__file__), "..", "static", "styles.css")) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

def setup_page():
    st.set_page_config(page_title="multi1 - Unified AI Reasoning Chains", page_icon="🧠", layout="wide")
    load_css()
    st.markdown("""
    <h1 class="main-title">
        🧠 multi1 - Unified AI Reasoning Chains
    </h1>
    """, unsafe_allow_html=True)
    st.markdown("""
    <p class="main-description">
        This app demonstrates AI reasoning chains using different backends: Ollama, Perplexity AI, and Groq.
        Choose a backend and enter your query to see the step-by-step reasoning process.
    </p>
    """, unsafe_allow_html=True)

def get_api_handler(backend, config):
    if backend == "Ollama":
        return OllamaHandler(config['OLLAMA_URL'], config['OLLAMA_MODEL'])
    elif backend == "Perplexity AI":
        return PerplexityHandler(config['PERPLEXITY_API_KEY'], config['PERPLEXITY_MODEL'])
    else:  # Groq
        return GroqHandler(config['GROQ_API_KEY'], config['GROQ_MODEL'])

def main():
    logger.info("Starting the application")
    setup_page()

    st.sidebar.markdown('<h3 class="sidebar-title">⚙️ Settings</h3>', unsafe_allow_html=True)
    config = config_menu()
    
    backend = st.sidebar.selectbox("Choose AI Backend", ["Ollama", "Perplexity AI", "Groq"])
    display_config(backend, config)
    api_handler = get_api_handler(backend, config)
    logger.info(f"Selected backend: {backend}")

    user_query = st.text_input("💬 Enter your query:", placeholder="e.g., How many 'R's are in the word strawberry?")

    if user_query:
        logger.info(f"Received user query: {user_query}")
        st.write("🔍 Generating response...")
        response_container = st.empty()
        time_container = st.empty()

        try:
            for steps, total_thinking_time in generate_response(user_query, api_handler):
                with response_container.container():
                    for title, content, _ in steps:
                        if title.startswith("Final Answer"):
                            st.markdown(f'<h3 class="expander-title">🎯 {title}</h3>', unsafe_allow_html=True)
                            st.markdown(f'<div>{content}</div>', unsafe_allow_html=True)
                            logger.info(f"Final answer generated: {content}")
                        else:
                            with st.expander(f"📝 {title}", expanded=True):
                                st.markdown(f'<div>{content}</div>', unsafe_allow_html=True)
                            logger.debug(f"Step completed: {title}")

                if total_thinking_time is not None:
                    time_container.markdown(f'<p class="thinking-time">⏱️ Total thinking time: {total_thinking_time:.2f} seconds</p>', unsafe_allow_html=True)
                    logger.info(f"Total thinking time: {total_thinking_time:.2f} seconds")
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}", exc_info=True)
            st.error("An error occurred while generating the response. Please try again.")

if __name__ == "__main__":
    main()
</app/main.py>

<app/utils.py>

import json
import time
import os


def generate_response(prompt, api_handler):
    messages = [
        {
            "role": "system",
            "content": """You are an expert AI assistant that explains your reasoning step by step. For each step, provide a title that describes what you're doing in that step, along with the content. Decide if you need another step or if you're ready to give the final answer. Respond in JSON format with 'title', 'content', and 'next_action' (either 'continue' or 'final_answer') keys. USE AS MANY REASONING STEPS AS POSSIBLE. AT LEAST 3. BE AWARE OF YOUR LIMITATIONS AS AN LLM AND WHAT YOU CAN AND CANNOT DO. IN YOUR REASONING, INCLUDE EXPLORATION OF ALTERNATIVE ANSWERS. CONSIDER YOU MAY BE WRONG, AND IF YOU ARE WRONG IN YOUR REASONING, WHERE IT WOULD BE. FULLY TEST ALL OTHER POSSIBILITIES. YOU CAN BE WRONG. WHEN YOU SAY YOU ARE RE-EXAMINING, ACTUALLY RE-EXAMINE, AND USE ANOTHER APPROACH TO DO SO. DO NOT JUST SAY YOU ARE RE-EXAMINING. USE AT LEAST 3 METHODS TO DERIVE THE ANSWER. USE BEST PRACTICES.""",
        },
        {"role": "user", "content": prompt},
        {
            "role": "assistant",
            "content": "Thank you! I will now think step by step following my instructions, starting at the beginning after decomposing the problem.",
        },
    ]

    steps = []
    step_count = 1
    total_thinking_time = 0

    while True:
        start_time = time.time()
        step_data = api_handler.make_api_call(messages, 300)
        end_time = time.time()
        thinking_time = end_time - start_time
        total_thinking_time += thinking_time

        steps.append(
            (
                f"Step {step_count}: {step_data['title']}",
                step_data["content"],
                thinking_time,
            )
        )

        messages.append({"role": "assistant", "content": json.dumps(step_data)})
        print("Next reasoning step: ", step_data["next_action"])
        if step_data["next_action"].lower().strip() == "final_answer":
            break

        step_count += 1

        yield steps, None

    messages.append(
        {
            "role": "user",
            "content": "Please provide the final answer based on your reasoning above.",
        }
    )

    start_time = time.time()
    final_data = api_handler.make_api_call(messages, 200, is_final_answer=True)
    end_time = time.time()
    thinking_time = end_time - start_time
    total_thinking_time += thinking_time

    steps.append(("Final Answer", final_data["content"], thinking_time))

    yield steps, total_thinking_time


def load_env_vars():
    return {
        "OLLAMA_URL": os.getenv("OLLAMA_URL", "http://localhost:11434"),
        "OLLAMA_MODEL": os.getenv("OLLAMA_MODEL", "llama3.1:70b"),
        "PERPLEXITY_API_KEY": os.getenv("PERPLEXITY_API_KEY"),
        "PERPLEXITY_MODEL": os.getenv(
            "PERPLEXITY_MODEL", "llama-3.1-sonar-small-128k-online"
        ),
    }
</app/utils.py>

<static/styles.css>
body {
    font-family: -apple-system, BlinkMacSystemFont, sans-serif;
}

h1, h2, h3, h4, h5, h6 {
    font-family: -apple-system, BlinkMacSystemFont, sans-serif;
}

.main-title {
    text-align: center;
}

.main-description {
    text-align: center;
    font-size: 1.1em;
}

.sidebar-title {
    font-family: -apple-system, BlinkMacSystemFont, sans-serif;
}

.expander-title {
    font-family: -apple-system, BlinkMacSystemFont, sans-serif;
}

.thinking-time {
    font-family: -apple-system, BlinkMacSystemFont, sans-serif;
    font-weight: bold;
}

@keyframes fadeIn {
    from { opacity: 0; }
    to { opacity: 1; }
}

@keyframes slideIn {
    from { transform: translateY(20px); opacity: 0; }
    to { transform: translateY(0); opacity: 1; }
}

@keyframes pulse {
    0% { transform: scale(1); }
    50% { transform: scale(1.05); }
    100% { transform: scale(1); }
}

/* Apply fade-in animation to main content */
.main .block-container {
    animation: fadeIn 0.5s ease-out;
}

/* Apply slide-in animation to expanders */
.streamlit-expanderHeader {
    animation: slideIn 0.3s ease-out;
    transition: background-color 0.3s ease;
}

/* Smooth transition for expander content */
.streamlit-expanderContent {
    transition: max-height 0.3s ease-out, opacity 0.3s ease-out;
}

/* Pulse animation for thinking time */
.thinking-time {
    animation: pulse 2s infinite;
}

/* Hover effect for buttons */
.stButton > button {
    transition: all 0.3s ease;
}

.stButton > button:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
}

/* Smooth transition for selectbox */
.stSelectbox {
    transition: all 0.3s ease;
}

/* Subtle hover effect for text input */
.stTextInput > div > div > input {
    transition: all 0.3s ease;
}

.stTextInput > div > div > input:hover {
    box-shadow: 0 0 0 1px rgba(49, 51, 63, 0.2);
}
</static/styles.css>

<README.md>

# multi1: Using multiple AI providers to create o1-like reasoning chains

***IMPORTANT: multi1 is a fork of [g1](https://github.com/bklieger-groq/g1/), made by [Benjamin Klieger](https://x.com/benjaminklieger). It was made as a way to experiment with multiple AI providers included local LLMs. All credits go to the original author.***

## Features

- [x] Using an unified interface to try out different providers
- [x] Configuring the app from the sidebar

## Providers

- [x] Ollama (local)
- [x] Perplexity (remote, requires API key)
- [x] Groq (remote, requires API key)

## Work in progress

- [ ] Add more providers

## Example

![Simple Math](examples/maths.png)

## Description

This is an early prototype of using prompting strategies to improve the LLM's reasoning capabilities through o1-like reasoning chains. This allows the LLM to "think" and solve logical problems that usually otherwise stump leading models. Unlike o1, all the reasoning tokens are shown, and the app uses an open source model.

multi1 is experimental and being open sourced to help inspire the open source community to develop new strategies to produce o1-like reasoning. This experiment helps show the power of prompting reasoning in visualized steps, not a comparison to or full replication of o1, which uses different techniques. OpenAI's o1 is instead trained with large-scale reinforcement learning to reason using Chain of Thought, achieving state-of-the-art performance on complex PhD-level problems.

multi1 demonstrates the potential of prompting alone to overcome straightforward LLM logic issues like the Strawberry problem, allowing existing open source models to benefit from dynamic reasoning chains and an improved interface for exploring them.


### How it works

multi1 powered by one of the supported models creates reasoning chains, in principle a dynamic Chain of Thought, that allows the LLM to "think" and solve some logical problems that usually otherwise stump leading models.

At each step, the LLM can choose to continue to another reasoning step, or provide a final answer. Each step is titled and visible to the user. The system prompt also includes tips for the LLM. There is a full explanation under Prompt Breakdown, but a few examples are asking the model to “include exploration of alternative answers” and “use at least 3 methods to derive the answer”.

The reasoning ability of the LLM is therefore improved through combining Chain-of-Thought with the requirement to try multiple methods, explore alternative answers, question previous draft solutions, and consider the LLM’s limitations. This alone, without any training, is sufficient to achieve ~70% accuracy on the Strawberry problem (n=10, "How many Rs are in strawberry?"). Without prompting, Llama-3.1-70b had 0% accuracy and ChatGPT-4o had 30% accuracy.


### Disclaimer

> [!IMPORTANT]
> multi1 is not perfect, but it can perform significantly better than LLMs out-of-the-box. From initial testing, multi1 accurately solves simple logic problems 60-80% of the time that usually stump LLMs. However, accuracy has yet to be formally evaluated. See examples below.



### Quickstart

To use the launcher, follow these instructions:

1. Set up the environment:

   ```
   python3 -m venv venv
   source venv/bin/activate
   pip3 install -r requirements.txt
   ```

2. Copy the example environment file:

   ```
   cp example.env .env
   ```

3. Edit the .env file with your API keys / models preferences (or do it from the app's configuration menu)

4. Run the main interface

   ```
   streamlit run app/main.py
   ```

---

### Prompting Strategy

The prompt is as follows:

```
You are an expert AI assistant that explains your reasoning step by step. For each step, provide a title that describes what you're doing in that step, along with the content. Decide if you need another step or if you're ready to give the final answer. Respond in JSON format with 'title', 'content', and 'next_action' (either 'continue' or 'final_answer') keys. USE AS MANY REASONING STEPS AS POSSIBLE. AT LEAST 3. BE AWARE OF YOUR LIMITATIONS AS AN LLM AND WHAT YOU CAN AND CANNOT DO. IN YOUR REASONING, INCLUDE EXPLORATION OF ALTERNATIVE ANSWERS. CONSIDER YOU MAY BE WRONG, AND IF YOU ARE WRONG IN YOUR REASONING, WHERE IT WOULD BE. FULLY TEST ALL OTHER POSSIBILITIES. YOU CAN BE WRONG. WHEN YOU SAY YOU ARE RE-EXAMINING, ACTUALLY RE-EXAMINE, AND USE ANOTHER APPROACH TO DO SO. DO NOT JUST SAY YOU ARE RE-EXAMINING. USE AT LEAST 3 METHODS TO DERIVE THE ANSWER. USE BEST PRACTICES.

Example of a valid JSON response:
json
{
    "title": "Identifying Key Information",
    "content": "To begin solving this problem, we need to carefully examine the given information and identify the crucial elements that will guide our solution process. This involves...",
    "next_action": "continue"
}
```

#### Breakdown

First, a persona is added:

> You are an expert AI assistant that explains your reasoning step by step.



Then, instructions to describe the expected step-by-step reasoning process while titling each reasoning step. This includes the ability for the LLM to decide if another reasoning step is needed or if the final answer can be provided.

> For each step, provide a title that describes what you're doing in that step, along with the content. Decide if you need another step or if you're ready to give the final answer.



JSON formatting is introduced with an example provided later.

> Respond in JSON format with 'title', 'content', and 'next_action' (either 'continue' or 'final_answer') keys.



In all-caps to improve prompt compliance by emphesizing the importance of the instruction, a set of tips and best practices are included.

1. Use as many reasoning steps as possible. At least 3. -> This ensures the LLM actually takes the time to think first, and results usually in about 5-10 steps.
2. Be aware of your limitations as an llm and what you can and cannot do. -> This helps the LLM remember to use techniques which produce better results, like breaking "strawberry" down into individual letters before counting.
3. Include exploration of alternative answers. Consider you may be wrong, and if you are wrong in your reasoning, where it would be. -> A large part of the gains seem to come from the LLM re-evaluating its initial response to ensure it logically aligns with the problem.
4. When you say you are re-examining, actually re-examine, and use another approach to do so. Do not just say you are re-examining. -> This encourages the prevention of the LLM just saying it re-examined a problem without actually trying a new approach.
5. Use at least 3 methods to derive the answer. -> This helps the LLM come to the right answer by trying multiple methods to derive it.
6. Use best practices. -> This is as simple as the "Do better" prompts which improve LLM code output. By telling the LLM to use best practices, or do better, it generally performs better!


> USE AS MANY REASONING STEPS AS POSSIBLE. AT LEAST 3. BE AWARE OF YOUR LIMITATIONS AS AN LLM AND WHAT YOU CAN AND CANNOT DO. IN YOUR REASONING, INCLUDE EXPLORATION OF ALTERNATIVE ANSWERS. CONSIDER YOU MAY BE WRONG, AND IF YOU ARE WRONG IN YOUR REASONING, WHERE IT WOULD BE. FULLY TEST ALL OTHER POSSIBILITIES. YOU CAN BE WRONG. WHEN YOU SAY YOU ARE RE-EXAMINING, ACTUALLY RE-EXAMINE, AND USE ANOTHER APPROACH TO DO SO. DO NOT JUST SAY YOU ARE RE-EXAMINING. USE AT LEAST 3 METHODS TO DERIVE THE ANSWER. USE BEST PRACTICES.



Finally, after the problem is added as a user message, an assistant message is loaded to provide a standardized starting point for the LLM's generation.

> Assistant: Thank you! I will now think step by step following my instructions, starting at the beginning after decomposing the problem


### Credits

g1 was originally developed by [Benjamin Klieger](https://x.com/benjaminklieger).
This multi1 fork was developed by [tcsenpai](https://github.com/tcsenpai).
</README.md>

<example.env>
GROQ_API_KEY=gsk...

OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:70b

PERPLEXITY_API_KEY=your_perplexity_api_key
PERPLEXITY_MODEL=llama-3.1-sonar-small-128k-online
</example.env>

<requirements.txt>
streamlit
groq
python-dotenv
requests
blessed
</requirements.txt>

</code_to_adapt>